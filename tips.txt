1. how to tune single model, especially xgboost
	most important hyperparameters for xgboost: eta, max_depth, col_sample_by_tree, gamma, 
	subsample, alpha, min_child_weight. Start with big eta, then decrease eta. Use early 
	stopping.
	https://www.kaggle.com/c/allstate-claims-severity/forums/t/24611/single-model-performance?limit=all

2. Bayesian optimization, hyperparameter tuning
	https://github.com/fmfn/BayesianOptimization

3. Useful kernels:
	parameters:
	https://www.kaggle.com/iglovikov/allstate-claims-severity/xgb-1114/code
	encoding:
	https://www.kaggle.com/modkzs/allstate-claims-severity/lexical-encoding-feature-comb/code
	NN:
	https://www.kaggle.com/mtinti/allstate-claims-severity/keras-starter-with-bagging-1111-84364/discussion
	stacking:
	https://www.kaggle.com/abhilashawasthi/allstate-claims-severity/stacking-starter/code
	bagging:
	https://www.kaggle.com/weimin/allstate-claims-severity/keras-starter-with-bagging-1111-84364
	xgboost cv:
	https://www.kaggle.com/mtinti/allstate-claims-severity/xgb-1110-from-vladimir-iglovikov-and-tilii7/comments

4. string as feature in tree:
	http://datascience.stackexchange.com/questions/5226/strings-as-features-in-decision-tree-random-forest/8458#8458

5. models to consider
	xgboost
	NN
	ridge

6. How to tune Keras
	https://www.kaggle.com/mtinti/allstate-claims-severity/keras-starter-with-bagging-1111-84364/comments

7. Kinect features
	https://www.kaggle.com/alexandrudaia/allstate-claims-severity/derived-kinetic-features/discussion

8. H2O R
	https://www.kaggle.com/tobikaggle/allstate-claims-severity/h2o-dnn-averaging-in-r/comments

9. Ensemble weight
	https://www.kaggle.com/tilii7/allstate-claims-severity/ensemble-weights-minimization-vs-mcmc/comments

10. TSNE
	https://www.kaggle.com/c/allstate-claims-severity/forums/t/26115/barnes-hut-tsne-part-iv-last-part